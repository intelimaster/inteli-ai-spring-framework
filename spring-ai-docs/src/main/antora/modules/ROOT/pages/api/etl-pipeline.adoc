= ETL Pipeline

The Extract, Transform, and Load (ETL) framework serves as the backbone of data processing within the Retrieval Augmented Generation (RAG) use case.

The ETL pipeline orchestrates the flow from raw data sources to a structured vector store, ensuring data is in the optimal format for retrieval by the AI model.

The RAG use case is text to augment the capabilities of generative models by retrieving relevant information from a body of data to enhance the quality and relevance of the generated output.


== API Overview

=== DocumentReader

Provides a source of documents from diverse origins.
```java
public interface DocumentReader extends Supplier<List<Document>> {

}
```

==== Available Implementations

*JsonReader*::
+ Parses documents in JSON format.

*TextReader*::
+ Processes plain text documents.

*PagePdfDocumentReader*::
+ Uses Apache PdfBox library to parse PDF documents

*ParagraphPdfDocumentReader*::
+ Uses the PDF catalog (e.g. TOC) information to split the input PDF into text paragraphs and output a single `Document` per paragraph.

*TikaDocumentReader*::
+ Uses Apache Tika to extract text from a variety of document formats, such as PDF, DOC/DOCX, PPT/PPTX, and HTML. For a comprehensive list of supported formats, refer to the  https://tika.apache.org/2.9.0/formats.html[Tika documentation].

=== DocumentTransformer

Transforms a batch of documents as part of the processing workflow.

```java
public interface DocumentTransformer extends Function<List<Document>, List<Document>> {

}
```

==== Available Implementations

*TextSplitter*::
+ Divides documents to fit the AI model's context window.

*TokenTextSplitter*::
+ Splits documents while preserving token-level integrity.

*ContentFormatTransformer*::
+ Ensures uniform content formats across all documents.

*KeywordMetadataEnricher*::
+ Augments documents with essential keyword metadata.

*SummaryMetadataEnricher*::
+ Enriches documents with summarization metadata for enhanced retrieval.

=== DocumentWriter

Manages the final stage of the ETL process, preparing documents for storage.

```java
public interface DocumentWriter extends Consumer<List<Document>> {

}
```

== Available Implementations

There is an implementation for each of the Vector Stores that Spring AI supports, e.g. `PineconeVectorStore`.

See xref:api/vectordbs.adoc[Vector DB Documentation] for a full listing.


== Using PagePdfDocumentReader

[source,java]
----
PagePdfDocumentReader pdfReader = new PagePdfDocumentReader(
	"file:document-readers/pdf-reader/src/test/resources/sample.pdf",
	PdfDocumentReaderConfig.builder()
			.withPageTopMargin(0)
			.withPageBottomMargin(0)
			.withPageExtractedTextFormatter(PageExtractedTextFormatter.builder()
					.withNumberOfTopTextLinesToDelete(0)
					.withNumberOfBottomTextLinesToDelete(3)
					.withNumberOfTopPagesToSkipBeforeDelete(0)
					.build())
			.withPagesPerDocument(1)
			.build());

var documents = pdfReader.get();

PdfTestUtils.writeToFile("document-readers/pdf-reader/target/sample.txt", documents, false);
----

[source,java]
----
public static void main(String[] args) throws IOException {

	ParagraphPdfDocumentReader pdfReader = new ParagraphPdfDocumentReader(
			"file:document-readers/pdf-reader/src/test/resources/sample2.pdf",
			PdfDocumentReaderConfig.builder()
					// .withPageBottomMargin(15)
					// .withReversedParagraphPosition(true)
					// .withTextLeftAlignment(true)
					.build());
	// ParagraphPdfDocumentReader pdfReader = new ParagraphPdfDocumentReader(
	// "file:document-readers/pdf-reader/src/test/resources/spring-framework.pdf",
	// PdfDocumentReaderConfig.builder()
	// .withPageBottomMargin(15)
	// .withReversedParagraphPosition(true)
	// // .withTextLeftAlignment(true)
	// .build());

	// PdfDocumentReader pdfReader = new
	// PdfDocumentReader("file:document-readers/pdf-reader/src/test/resources/uber-k-10.pdf",
	// PdfDocumentReaderConfig.builder().withPageTopMargin(80).withPageBottomMargin(60).build());

	var documents = pdfReader.get();

	writeToFile("document-readers/pdf-reader/target/sample2.txt", documents, true);
	System.out.println(documents.size());

}
----
